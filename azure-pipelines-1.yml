trigger:
- main  # Trigger the pipeline when changes are pushed to the 'main' branch

pool:
  vmImage: 'ubuntu-latest'  # Use an Ubuntu-based agent to run the pipeline

variables:
  # Databricks workspace URL and token will be taken from the pipeline's secret variables
  databricksWorkspaceUrl: 'https://adb-4276282781475924.4.azuredatabricks.net'  # Your Databricks Workspace URL
  databricksToken: $(DATABRICKS_TOKEN)  # Databricks token stored as a secret in Azure DevOps
  databricksFolderPath: '/Workspace/Sample_Notebook'  # Databricks Workspace path for the notebook
  notebookFilePath: 'Sample-Notebook.py'  # Path to the notebook file in the repository

jobs:
- job: DeployToDatabricks
  displayName: 'Deploy Notebook to Databricks Workspace'
  
  steps:
  # Step 1: Checkout the code from GitHub repository
  - checkout: self
    displayName: 'Checkout Code from GitHub'

  # Step 2: Install Python and Databricks CLI
  - task: UsePythonVersion@0
    inputs:
      versionSpec: '3.9'
    displayName: 'Use Python 3.9'

  - script: |
      pip install databricks-cli
    displayName: 'Install Databricks CLI'

  # Step 3: Configure Databricks CLI with token and workspace URL
  - script: |
      echo "$(databricksWorkspaceUrl)"
      echo "$(databricksToken)"
      databricks configure --token <<EOF
      $(databricksWorkspaceUrl)
      $(databricksToken)
      EOF
    displayName: 'Configure Databricks CLI'

  # Step 4: Upload the notebook to Databricks workspace
  - script: |
      databricks workspace import ${{ variables.notebookFilePath }} ${{ variables.databricksFolderPath }}/Sample-Notebook.py --overwrite
    displayName: 'Upload Notebook to Databricks Workspace'

  # Step 5: Indicate success
  - script: |
      echo "Notebook deployed to Databricks Workspace successfully."
    displayName: 'Deployment Successful'
